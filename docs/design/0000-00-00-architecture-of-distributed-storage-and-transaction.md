# Architecture Of Distributed Storage And Transaction

- Author: [Zhigao Tong](http://github.com/solotzg)

## Introduction

There is already a distributed `OLTP`(Online transaction processing) storage product [TiKV](https://github.com/tikv/tikv).
In order to make TiDB applicable to `HTAP`(Hybrid transaction/analytical processing) scenario, we need another storage system called `TiFlash`, to enhance the ability of realtime analytics.
Unlike other typical `OLAP`(Online analytical processing) databases which only guarantee `Eventual Consistency`, TiFlash is built on same distributed infrastructure(`Multi-raft RSM`, `Percolator Transaction Model`) like TiKV and plays the same role(`raft store`) in cluster as well.
TiFlash is designed to provide `Strong Consistency` read services, which means
any other component, like TiDB or TiSpark, is able to access TiFlash and TiKV by same `Coprocessor Protocol`.

## Design

### Overview

![tiflash-overall](./images/tiflash-overall.svg)

After one TiFlash node deployed into TiDB cluster, it will register necessary properties(labels with `engine`:`tiflash`, address, runtime information, etc.) to [Placement driver](https://github.com/tikv/pd)(hereafter referred to as `PD`) as a `raft store`.

PD must NOT schedule any region peer to TiFlash store if there is no [Placement Rules](https://docs.pingcap.com/tidb/stable/configure-placement-rules) set by TiFlash.
After TiDB has executed the `DDL` job which tries to set `TIFLASH REPLICA` for specific table, `Replica Manager` module will translate related jobs into `Placement Rules` and update to PD.
Then, PD will split and schedule `Learner` peer of related regions to TiFlash store by corresponding rules.

All peers of same region make up a `Raft Group`.
The group represents a `Replicated State Machine`(RSM).
The `Leader` replicates actions to the followers and learners.
Each peer has a `Durable Write Ahead Log`(DWAL).
All peers append each action as an entry in the log immediately as they recieve it.
When the quorum (the majority) of peers has confirmed that the entry exists in their log, the leader commits the log, each peer then can apply the action to their state machine.
TiFlash relies on the [TiDB Engine Extensions Library](https://github.com/pingcap/tidb-engine-ext)(works as `Raft Store` dynamic library) to maintain multi-raft RSM.
A `TIFLASH REPLICA` of table is an abstract concept which can be regarded as a collection of multiple learner peers(region range of peer intersects with record data range of the table) in TiFlash store.

A database transaction, by definition, must be atomic, consistent, isolated and durable.
Transaction writing proposed by TiDB must follow `Percolator Model`.
Key-value engine of TiKV provides a feature named Column Family (hereafter referred to as CF).
The three CFs: `DEFAULT`, `LOCK` and `WRITE`, correspond to Percolator's `data column`, `lock column` and `write column` respectively.
TiFlash builds abstract layers of `Region` from `Raft Store` and applies raft commands with writing operations towards these CFs.
TiFlash fetches `Table Schema`(generated by TiDB) from TiKV and try to write the committed records into strong schema-aware column storage.

`Coprocessor Protocol` is the basic protocol used to read table data or execute sub-tasks in `raft store`.
`Region Peer` is the minimal unit for transaction reading.
To guarantee `Snapshot Isolation`, there are a few important safeguard mechanisms:

- `Replica Read` makes sure that the raft state machine of region peers are correct and have enough context.
- `Resolve Lock` checks whether related table records are protected by locks and tries to resolve them.
- `MVCC(Multiversion concurrency control)` read table records by specific version presented by `TSO(Timestamp Oracle)`.

### Distributed Storage Framework

![tiflash-distributed-architecture](./images/tiflash-distributed-architecture.svg)

[TiDB Engine Extensions Library](https://github.com/pingcap/tidb-engine-ext)(hereafter referred to as `raftstore-proxy` or `tiflash-proxy`) is a TiKV based `c dynamic library` for extending storage system in `TiDB` cluster.
This library aims to export current multi-raft framework to other engines and make them be able to provide services(read/write) as `raft store` directly.

Generally speaking, there are two storage components, `RaftEngine` and `KvEngine`, in TiKV for maintaining multi-raft RSM.
KvEngine is mainly used for applying raft command and providing key-value services.
Each time raft log has been committed in RaftEngine, it will be parsed into normal/admin raft command and be handled by apply process.
Multiple modifications about region data/meta/apply-state will be encapsulated into one `Write Batch` and written into KvEngine atomically.
Maybe replacing KvEngine by `Engine Traits` is an option.
But for other storage system, it's not easy to guarantee atomicity while writing/reading dynamic key-value pair(such as meta/apply-state) and patterned data(strong schema) together.
Besides, there are a few modules and components(like importer or lighting) reply on the sst format of TiKV.
It may take a lot of cost to achieve such replacing.

In order to bring few intrusive modifications against original logic of TiKV, it's suggested to let apply process work as usual but only persist meta and state information.
It means that, each place where may write normal region data, must be replaced with related interfaces.
Not like KvEngine, storage system(called `engine-store`) under such framework should be aware of transition about multi-raft state machine from these interfaces.
`engine-store` must have the ability of dealing with raft commands, so as to handle queries with region epoch.

`region snapshot` presents the whole information of a region(data/meta/apply-state) at a specific apply-state.

Anyway, because there are at least two asynchronous runtimes in one program, the best practice of such raft store is to guarantee `External Consistency` by `region snapshot`.
Actually, the raft logs persisted in RaftEngine are the `WAL(Write Ahead Log)` of apply process.
Index of raft entry within same region peer is monotonic increasing.
If process is interrupted at middle step, it should replay from last persisted apply-state after restarted.
Related modifications cannot be witnessed from outside until meets safe-point.

`Idempotency` is an important property for external consistency, which means such system could handle outdated raft commands. A practical way is like:

- Fsync snapshot in `engine-store` atomically
- Fsync region snapshot in `raftstore-proxy` atomically
- Make RaftEngine only gc raft log whose index is smaller than persisted apply-state
- `engine-store` should tell and ignore raft commands with outdated apply-state during apply process
- `engine-store` should recover from middle step by overwriting and must NOT provide services until caught up with latest state

Such architecture inherited several important features from TiKV naturally, such as distributed fault tolerance/recovery, automatic re-balancing, etc.
It's also convenient for PD to maintain this kind of storage system by existing way as long as it works as `raft store`.
Reserved label with key `engine`(defined when compiling) is definitely forbidden in configuration.
Components in same cluster can use it to tell engine type of store.

#### Interfaces

Since the program language `Rust`, which TiKV is based on, has zero-cost abstractions.
It's very easy to let different threads interact with each other by `FFI`(Foreign Function Interface).
Such mode brings almost no overhead.
However, any caller must be quite clear about the boundary of safe/unsafe operations exactly.
The structure used by different runtimes through interfaces must have same memory layout.

It's feasible to refactor TiKV source code and extract parts of necessary process into interfaces. The main categories are like:

- apply normal-write raft command
- apply admin raft command
- peer detect: destroy peer
- snapshot: pre-handle/apply region snapshot
- sst file reader
- apply ingest-sst command
- replica read: batch read-index
- encryption: get file; new file; delete file; link file; rename file;
- status services: metrics; cpu profile; get config; thread stats; self-defined api;
- store stats: key/bytes R/W stats; disk stats; storage `engine-store` stats;
- tools/utils

TiKV can perform split or merge on regions to make the partitions more flexible.
When the size of a region exceeds the limit, it will be divided into two or more regions, and the range may change like `[a, c) -> [a, b) + [b, c)`.
When the sizes of two sibling regions are small enough, they will be merged into a bigger region, and the range may change like `[a, b) + [b, c) -> [a, c)`.

To apply admin raft command safely, region snapshot must be persisted when executing commands about `split`, `merge` and `change peer`.
Because such commands will change the core properties of multi-raft RSM, such as `version`, `conf version`, `start/end key`.
Ignorable admin command `CompactLog` may trigger raft log gc in `RaftEngine`.
So it's required to persist region snapshot if decide to execute such command.
But for normal-write, the decision of persisting can be pushed down to `engine-store`.

When the region in current store is illegal or pending remove, the task about `destroy-peer` will be executed to clean useless data.

![txn-log-replication](./images/txn-log-replication.svg)

According to the basic transaction log replication, each writing action must be committed or applied by leader peer before returning success ack to client.
When any peer tries to respond queries, it should get latest committed-index from leader and wait until apply-state caught up, in order to make sure it has had enough context.
No matter for learner/follower or even leader, `Read Index` is an effective choice to check latest `Lease`.
Because it's easy to make any peer of region group provide read service under same logic as long as overhead of read-index itself is insignificant.

When leader peer has gc raft log or other peers can not proceed RSM in current context, other peers can request region snapshot from leader.
However, the region snapshot data, which in form of TiKV `SST` file, is really hard to used by other storage system directly.
To accelerate the speed of applying region snapshot data, the normal process has been separated into several parts:

- `SST File Reader` to read key-value one by one from sst files.
- Multi-thread pool to pre-handle sst files into self-defined structure of `engine-store`.
- Apply self-defined structure by original sequence

Interfaces about `IngestSst` is the core to be compatible with `TiDB Lighting` and `BR` for `HTAP` scenario.
It can substantially speed up data loading/restoring.
`SST File Reader` is also quite useful while applying `IngestSst` raft command.

Encryption is an important feature for `DBaaS`(database as a service).
To be compatible with TiKV, a data key manager with same logic is indispensable, especially for rotating data encryption key or using KMS service.

Status services like metrics, cpu/mem profile(flame graph) or other self-defined stats can provide effective support for diagnosis.
It's suggested to encapsulate those into one status server and let other external components visit through status address.
Most of original metrics of TiKV could be reused and an optional way is to add specific prefix for each name.

To reduce IOPS(mainly in RaftEngine) and make such system friendly for poor platform, when maintaining DWAL, it's effective to batch raft msg before fsync as long as latency is tolerable.

### Transaction

Basic synopsis: [TiDB Transaction](https://pingcap.github.io/tidb-dev-guide/understand-tidb/transaction.html), [Optimized Percolator](https://tikv.org/deep-dive/distributed-transaction/optimized-percolator)

#### Multi-raft RSM

To provide services as a `raft store` directly, TiFlash must implement all interfaces provided by raftstore-proxy.
There are two kinds of raft command: `normal-write`, `admin`.

**normal-write command** consists of [cmd types](https://tikv.github.io/doc/kvproto/raft_cmdpb/enum.CmdType.html) and key-value pairs towards CFs.
For now, except cmd type `Put`, `Delete`, `DeleteRange` or `IngestSst`, others are useless.

- `DeleteRange` is ignorable becuase such type only appears when table is dropped safely(exceed gc safe time) but TiFlash has its own table gc strategy to clean data directly.
- `Put` means replace into a key-value
- `Delete` means delete key-value by key
- `IngestSst` means ingest several TiKV sst files of DEFAULT/WRITE CFs.

The content of each CF:

- `DEFAULT`: `(key, start_ts)` -> `value`
- `LOCK`: `key` -> `lock_info(lock_type, primary_lock, lock_version, ...)`
- `WRITE`: `(key, commit_ts)` -> `write_info(write_type, start_ts, [short_value], ...)`

In concept of `Snapshot Isolation`, all deletion operations proposed by transaction process are logical, which means `Put` a key-value with write_type `D`(delete).
The whole transaction process is quite complex, but for TiFlash which used for reading, except write_type `P` or `D`, others like `L`(lock) or `R`(rollback) can be ignored safely.
TiFlash will try to find committed data(legal values in WRITE CF) and transform related message into complete table records by schema.

**admin command** is one of [AdminCmdType](https://tikv.github.io/doc/kvproto/raft_cmdpb/enum.AdminCmdType.html).
In most of FFI functions, raftstore-proxy will add apply result(include region meta) as a parameter.
TiFlash can use such parameter to maintain RSM directly.
To reduce IOPS and make table records flushed into column storage by large batch(friendly for column storage), another mechanism(takes timeout, write-throughput, size, etc. into account) is designed to reduce frequency of executing `CompactLog` command.

**apply region snapshot** functions aim to transform TiKV's region snapshot into TiFlash's.
Before actually writing data into column storage, old data within [start-key, end-key) of new region must be strictly deleted.

##### Region Snapshot

Unlike TiKV, which uses [RocksDB](https://github.com/tikv/rocksdb) as KvEngine directly, TiFlash can maintain multi-raft RSM in several parts:

- RSM in raftstore-proxy
  - Build real RSM like TiKV but no actual info in data CFs.
  - Communicate with other components as a raftstore.
  - Expose region meta, data and other necessary info to TiFlash.
- Region Snapshot Persister
  - Persist whole region cache in memory by apply-state as region snapshot atomically.
- Committed table records in column storage
  - Store committed transaction data by strong schema.
  - Support MVCC by tso
  - Support table data GC

Redundancy is a practical way to guarantee `Idempotency` and `External Consistency`, which means updating region meta should be lazy while adding data but advanced while removing region.

Persisting whole region cache as snapshot can help to avoid using `WAL` but might bring more overhead when there are lots of uncommitted data.
It may happen if somehow a quite large transaction is blocked for a long time.
But for most normal scenarios, transaction will be committed or rollbacked quickly, which means the frequent operations towards LOCK/DEFAULT won't cause excessive remained data in memory.

To solve such problem, an optional way is to implement incremental store mode, which needs to use another key-value storage or other semi-structured engine as intermediate buffers.

#### Learner Read

After the feature [Async Commit](https://pingcap.github.io/tidb-dev-guide/understand-tidb/async-commit.html) and [1PC](https://pingcap.github.io/tidb-dev-guide/understand-tidb/1pc.html), `Read Index` request should contain start-ts of transaction read to resolve memory locks of leader peer in TiKV.
After current region peer has applied to latest committed index, it's available to check table locks(like TiKV does) and try to resolve them.

Epoch(`version`, `conf version`) is one of important properties to present region meta changing.
Latest `GC Safe Point` should always be smaller than start-ts of transaction read.
Both of them shall be double checked even after getting immutable snapshot information from storage.

The logic about **Resolve Lock** is complex.
Related behaviors should follow the established process(like TiDB does) in different [Clients](https://github.com/tikv?q=client&type=all).
No detail will be discussed here.

## Notice

To understand the architecture shown above, please learn those first:

- Source code about raftstore, rocksdb, transaction modules in [TiKV](https://github.com/tikv/tikv)
- Source code about DDL, transaction modules in [TiDB](https://github.com/pingcap/tidb)
- `Placement Rules`, scheduler modules in [PD](https://github.com/tikv/pd)
